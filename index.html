<!DOCTYPE html>
<html lang="en">
<head>
<meta charset="UTF-8">
<title>YOLOv8 TF.js Demo</title>
<style>
  video, canvas { border: 1px solid black; }
</style>
<script src="https://cdn.jsdelivr.net/npm/@tensorflow/tfjs@4.22.0/dist/tf.min.js"></script>
</head>
<body>
<h2>YOLOv8 TF.js Demo</h2>
<button id="switchBtn">切换摄像头</button>
<br><br>
<video id="video" autoplay playsinline></video>
<canvas id="canvas"></canvas>

<script>
const confThreshold = 0.25; // 最低置信度
const iouThreshold = 0.45;  // NMS 阈值

window.onload = async () => {
  let model;
  let video = document.getElementById('video');
  let canvas = document.getElementById('canvas');
  let ctx = canvas.getContext('2d');
  let useFrontCamera = false;
  let currentStream;

  // 加载模型
  async function loadModel() {
    model = await tf.loadGraphModel('https://pinocchione.github.io/ios-yolo-demo/yolov8n_web_model/model.json');
    console.log('模型加载完成');
  }

  // 启动摄像头
  async function startCamera() {
    if (currentStream) currentStream.getTracks().forEach(t=>t.stop());
    const constraints = { audio: false, video: { facingMode: useFrontCamera ? 'user' : 'environment' } };
    currentStream = await navigator.mediaDevices.getUserMedia(constraints);
    video.srcObject = currentStream;

    video.onloadedmetadata = () => {
      video.play();
      canvas.width = 640;
      canvas.height = 640;
      detectFrame();
    };
  }

  document.getElementById('switchBtn').addEventListener('click', () => {
    useFrontCamera = !useFrontCamera;
    startCamera();
  });

  // 画框函数
  function drawBoxes(boxes) {
    ctx.clearRect(0, 0, canvas.width, canvas.height);
    ctx.drawImage(video, 0, 0, canvas.width, canvas.height);
    boxes.forEach(b => {
      ctx.strokeStyle = 'red';
      ctx.lineWidth = 2;
      ctx.strokeRect(b[0], b[1], b[2]-b[0], b[3]-b[1]);
      ctx.fillStyle = 'red';
      ctx.font = '16px Arial';
      ctx.fillText(`${b[5]} ${b[4].toFixed(2)}`, b[0], b[1]-5);
    });
  }

  // YOLO输出解析 + NMS
  async function processOutput(pred) {
    const data = pred.dataSync();
    const numBoxes = data.length / 84;
    const boxes = [];
    const scores = [];
    const classes = [];

    for (let i = 0; i < numBoxes; i++) {
      const offset = i * 84;
      const x = data[offset];
      const y = data[offset+1];
      const w = data[offset+2];
      const h = data[offset+3];
      const objScore = data[offset+4];
      const classProbs = data.slice(offset+5, offset+84);
      const classId = classProbs.indexOf(Math.max(...classProbs));
      const classScore = classProbs[classId];
      const score = objScore * classScore;

      if (score > confThreshold) {
        // 转换为左上右下坐标
        const x1 = Math.max(0, (x - w/2) * 640);
        const y1 = Math.max(0, (y - h/2) * 640);
        const x2 = Math.min(640, (x + w/2) * 640);
        const y2 = Math.min(640, (y + h/2) * 640);
        boxes.push([y1, x1, y2, x2]); // tf.image.nonMaxSuppression 要求[y1,x1,y2,x2]
        scores.push(score);
        classes.push(classId);
      }
    }

    if (boxes.length === 0) return [];

    const tfBoxes = tf.tensor2d(boxes);
    const tfScores = tf.tensor1d(scores);
    const selected = await tf.image.nonMaxSuppressionAsync(tfBoxes, tfScores, 50, iouThreshold, confThreshold);

    const selectedBoxes = selected.arraySync().map(i => {
      const b = boxes[i];
      return [b[1], b[0], b[3], b[2], scores[i], classes[i]]; // [x1,y1,x2,y2,score,classId]
    });

    tfBoxes.dispose();
    tfScores.dispose();
    selected.dispose();

    return selectedBoxes;
  }

  async function detectFrame() {
    if (!model) return requestAnimationFrame(detectFrame);

    tf.engine().startScope();
    const input = tf.browser.fromPixels(video).resizeBilinear([640,640]).expandDims(0).toFloat();
    const pred = await model.executeAsync({x: input});
    const boxes = await processOutput(pred);
    drawBoxes(boxes);
    tf.engine().endScope();

    requestAnimationFrame(detectFrame);
  }

  await loadModel();
  await startCamera();
};
</script>
</body>
</html>
